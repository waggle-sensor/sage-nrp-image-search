apiVersion: apps/v1
kind: Deployment
metadata:
  name: benchmark-evaluator
  labels:
    app: benchmark-evaluator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: benchmark-evaluator
  template:
    metadata:
      labels:
        app: benchmark-evaluator
    spec:
      containers:
        - name: benchmark-evaluator
          image: PLACEHOLDER_BENCHMARK_EVALUATOR_IMAGE:latest
          # Note: Vector DB and inference server environment variables should be
          # added via patches in benchmark-specific overlays (e.g., env.yaml)
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: PYTHONPATH
              value: "/app"
          resources:
            limits:
              cpu: 4
              memory: 8Gi
            requests:
              cpu: 2
              memory: 4Gi
          volumeMounts:
            - name: huggingface-cache
              mountPath: /root/.cache/huggingface
            - name: results
              mountPath: /app/results
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - "ps aux | grep '[p]ython main.py' || exit 1"
            initialDelaySeconds: 60
            periodSeconds: 30
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - "test -f /app/.ready || exit 1"
            initialDelaySeconds: 30
            periodSeconds: 10
      volumes:
        - name: huggingface-cache
          persistentVolumeClaim:
            claimName: huggingface-pvc
        - name: results
          persistentVolumeClaim:
            claimName: results-pvc
      restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: benchmark-evaluator
  labels:
    app: benchmark-evaluator
spec:
  selector:
    app: benchmark-evaluator
  ports:
    - name: http
      port: 8080
      targetPort: 8080
  type: ClusterIP

